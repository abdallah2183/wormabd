import os
import sys
import io
import time
import json
import atexit
import signal
import torch
from flask import Flask, request, jsonify, send_file
from diffusers import StableDiffusionPipeline, StableDiffusionXLPipeline
from dotenv import load_dotenv
import threading
import time
import secrets # Import secrets for image file naming
from huggingface_hub import hf_hub_download # <-- NEW IMPORT

# Force CPU usage and disable CUDA
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'
torch.backends.cuda.enabled = False
torch.cuda.is_available = lambda: False
torch.cuda.device_count = lambda: 0

# Set device to CPU
DEVICE = 'cpu'

# Disable unnecessary logging
import logging
logging.basicConfig(level=logging.INFO)
logging.getLogger('diffusers').setLevel(logging.WARNING)
logging.getLogger('transformers').setLevel(logging.WARNING)

# Load environment variables
load_dotenv()

# =======================================================
# ðŸ“Œ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¹Ù„Ù‰ Hugging Face Hub (ÙŠØ¬Ø¨ ØªØ¹Ø¯ÙŠÙ„Ù‡Ø§!)
# =======================================================
# Ø§Ù„Ø±Ø¬Ø§Ø¡ ØªØºÙŠÙŠØ± Ù‡Ø°Ø§ Ø§Ù„Ù…ØªØºÙŠØ± Ù„ÙŠØ·Ø§Ø¨Ù‚ Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙˆØ§Ø³Ù… Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ Ø¹Ù„Ù‰ Hugging Face
HF_REPO_ID = os.environ.get("HF_REPO_ID", "YourName/YourModelRepo") # <--- Ø¹Ø¯Ù‘Ù„ Ù‡Ø°Ø§!
# =======================================================

# Configuration
CONFIG = {
    'HOST': '127.0.0.1',
    'PORT': 5000,
    'API_KEYS': [os.getenv('AI_SERVER_API_KEY', 'My_Website_Secure_Key_123456')],
    # Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ù‡Ù†Ø§ Ø³ØªÙƒÙˆÙ† Ù…Ø³Ø§Ø±Ø§Øª Ù…Ø¤Ù‚ØªØ© ÙŠØªÙ… ØªØ­Ø¯ÙŠØ«Ù‡Ø§ Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ù† Hugging Face
    'MODEL_PATHS': { 
        'text': 'model_files/dolphin-2.9-llama3-8b-q8_0.gguf',
        'image': 'model_files/Juggernaut-XL_v9_RunDiffusionPhoto_v2.safetensors'
    },
    'MAX_TOKENS': 1024,
    'DEFAULT_IMAGE_SIZE': 512,
    'MAX_IMAGE_SIZE': 1024
}

# Initialize Flask app (Single initialization, use 'app' variable)
app = Flask(__name__)
app.config['JSON_AS_ASCII'] = False # For proper Arabic text support

# Load environment variables
YOUR_API_KEY = os.getenv('AI_SERVER_API_KEY', 'My_Website_Secure_Key_123456')
# Fix Windows console encoding (might not be strictly needed on Linux, but harmless)
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# Initialize global variables
llama_model = None
sd_pipeline = None

# Ensure model directory exists (No longer needed since we use cache)
# os.makedirs('model_files', exist_ok=True) 

# Disable gradient calculation for inference
torch.set_grad_enabled(False)

# Force CPU usage
torch.set_default_tensor_type(torch.FloatTensor)
DEVICE = 'cpu'

# --- Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù†ÙˆØ§Ø© ÙˆØ§Ù„ØªØ¨Ø¹ÙŠØ§Øª ---
try:
    from llama_cpp import Llama
    # from diffusers import StableDiffusionPipeline # Already imported at the top
    # import torch # Already imported at the top
except ImportError as e:
    print(f"ðŸš¨ Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª: {e}")
    pass 

# ØªØ­Ù…ÙŠÙ„ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© (Ù…Ø«Ù„ Ù…ÙØªØ§Ø­Ùƒ Ø§Ù„Ø³Ø±ÙŠ)
load_dotenv(".env")

# --- Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø³Ø±ÙŠØ© ÙˆØ§Ù„Ù…Ø³Ø§Ø±Ø§Øª ---
# Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© Ù„Ù… ØªØ¹Ø¯ ØªØ³ØªØ®Ø¯Ù… Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ Ø¨Ù„ ØªØ³ØªØ®Ø¯Ù… ÙƒÙ…Ø¹Ø±ÙØ§Øª Ù„Ù„Ù…Ù„ÙØ§Øª ÙÙ‚Ø·
YOUR_API_KEY = os.environ.get("AI_SERVER_API_KEY")
TEXT_MODEL_FILENAME = os.environ.get("TEXT_MODEL", "dolphin-2.9-llama3-8b-q8_0.gguf")
IMAGE_MODEL_FILENAME = os.environ.get("IMAGE_MODEL", "Juggernaut-XL_v9_RunDiffusionPhoto_v2.safetensors")

# ØªÙ… Ø­Ø°Ù Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…Ø­Ù„ÙŠØ©
# if not os.path.exists(D_MODEL_PATH): ...

# --- Ø¯Ø§Ù„Ø© Ø§Ù„Ù…ØµØ§Ø¯Ù‚Ø© (Ø§Ù„Ø­Ø§Ø±Ø³) ---
def authenticate_request():
    """Verify API key from request headers"""
    api_key = request.headers.get('X-API-Key')
    if not api_key or api_key not in CONFIG['API_KEYS']:
        return False, jsonify({
            "status": "error",
            "message": "Invalid or missing API key"
        }), 401
    return True, None, None

def cleanup_models():
    """Clean up model resources properly"""
    global llama_model, sd_pipeline
    
    if llama_model is not None:
        try:
            # Simple dereference (llama_cpp should handle its own memory)
            llama_model = None
        except Exception as e:
            print(f"[WARNING] Error cleaning up text model: {e}")
    
    if sd_pipeline is not None:
        try:
            # Dereference the pipeline object
            sd_pipeline = None
            # Aggressively clear PyTorch cache if possible
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        except Exception as e:
            print(f"[WARNING] Error cleaning up image model: {e}")


# --- ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†ÙˆØ§ØªÙŠÙ† Ø¹Ù†Ø¯ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø®Ø§Ø¯Ù… ---
def load_ai_cores():
    """Load all AI models with proper error handling"""
    global llama_model, sd_pipeline
    
    models_loaded = True
    
    # Clean up existing models
    cleanup_models()
    
    print(f"\n{'='*50}")
    print(f"WORMGPT Server - Running on {DEVICE.upper()}")
    print(f"Hugging Face Repo ID: {HF_REPO_ID}")
    print(f"{'='*50}\n")
    
    # =============================================
    # â¬‡ï¸ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ù† Hugging Face Hub (Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù†ØµÙŠ)
    # =============================================
    text_model_local_path = None
    try:
        print(f"[TEXT] Downloading {TEXT_MODEL_FILENAME} from Hugging Face...")
        text_model_local_path = hf_hub_download(
            repo_id=HF_REPO_ID,
            filename=TEXT_MODEL_FILENAME,
            revision="main", # ÙŠÙ…ÙƒÙ† ØªØºÙŠÙŠØ±Ù‡Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø¹Ù„Ù‰ ÙØ±Ø¹ Ø¢Ø®Ø±
            cache_dir="./hf_cache" # Ù…Ø³Ø§Ø± Ù„Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        )
        CONFIG['MODEL_PATHS']['text'] = text_model_local_path # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø³Ø§Ø± Ù„Ù„Ù‚Ø±Ø§Ø¡Ø© Ù…Ù† Ø§Ù„Ù‚Ø±Øµ
        print(f"[TEXT] âœ… Model downloaded successfully to: {text_model_local_path}")
    except Exception as e:
        print(f"[ERROR] âŒ Failed to download text model from Hugging Face: {str(e)}")
        models_loaded = False
        
    # =============================================
    # â¬‡ï¸ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ù† Hugging Face Hub (Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØµÙˆØ±)
    # =============================================
    image_model_local_path = None
    try:
        print(f"\n[IMAGE] Downloading {IMAGE_MODEL_FILENAME} from Hugging Face...")
        image_model_local_path = hf_hub_download(
            repo_id=HF_REPO_ID,
            filename=IMAGE_MODEL_FILENAME,
            revision="main",
            cache_dir="./hf_cache"
        )
        CONFIG['MODEL_PATHS']['image'] = image_model_local_path # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø³Ø§Ø±
        print(f"[IMAGE] âœ… Model downloaded successfully to: {image_model_local_path}")
    except Exception as e:
        print(f"[ERROR] âŒ Failed to download image model from Hugging Face: {str(e)}")
        # Ø§Ù„Ø³Ù…Ø§Ø­ Ù„Ù„Ø®Ø§Ø¯Ù… Ø¨Ø§Ù„Ø§Ø³ØªÙ…Ø±Ø§Ø± Ø¨Ø¯ÙˆÙ† Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØµÙˆØ±
    
    # Load text generation model
    if text_model_local_path:
        try:
            from llama_cpp import Llama
            print("\n[TEXT] Loading language model...")
            llama_model = Llama(
                model_path=CONFIG['MODEL_PATHS']['text'],
                n_ctx=4096,
                n_threads=os.cpu_count() // 2,
                verbose=False,
                n_gpu_layers=0 # Force CPU
            )
            print("[TEXT] âœ… Model loaded successfully!")
        except Exception as e:
            print(f"[ERROR] âŒ Failed to load text model: {str(e)}")
            models_loaded = False
    else:
        print("[TEXT] â„¹ï¸ Skipping text model loading due to prior download failure.")
        models_loaded = False
        
    # Load image generation model (optional)
    if image_model_local_path:
        try:
            print("\n[IMAGE] Loading image generation model...")
            
            # Configure model loading
            load_kwargs = {
                'torch_dtype': torch.float32,
                'safety_checker': None,
                'requires_safety_checker': False,
                # local_files_only=True remains, but now points to the downloaded cache path
                'local_files_only': True, 
                'use_safetensors': True,
                'variant': 'fp32',
            }
            
            # Load the model on CPU
            with torch.device('cpu'):
                # Force CPU even if CUDA is detected
                torch.cuda.is_available = lambda: False
                sd_pipeline = StableDiffusionXLPipeline.from_single_file(
                    CONFIG['MODEL_PATHS']['image'],
                    torch_dtype=torch.float32,
                    safety_checker=None,
                    requires_safety_checker=False,
                    local_files_only=True,
                    use_safetensors=True,
                    variant='fp32'
                ).to('cpu')
                
                # Test the model with a simple prompt
                print("[IMAGE] Testing model with simple prompt...")
                with torch.no_grad():
                    # Reduce test size to prevent memory crash during startup
                    test_output = sd_pipeline(
                        prompt="test",
                        num_inference_steps=1,
                        width=64,
                        height=64,
                        output_type="pil",
                        generator=torch.Generator(device=DEVICE)
                    )
                
                print("[IMAGE] âœ… Model loaded and tested successfully!")
                
        except Exception as e:
            print(f"[ERROR] âŒ Failed to load image model: {str(e)}")
            print("[IMAGE] â„¹ï¸ Â Image generation will be disabled")
            import traceback
            traceback.print_exc()
            sd_pipeline = None
    else:
        print("[IMAGE] â„¹ï¸ Skipping image model loading due to prior download failure.")
        sd_pipeline = None
    
    return models_loaded


# Execute model loading once when the module is imported by Gunicorn
# This replaces the logic that was inside if __name__ == '__main__':
print("[INFO] Starting AI server setup (Gunicorn launch)...")
try:
    models_loaded = load_ai_cores()
    if models_loaded:
        print("[SUCCESS] AI Core Initialization Complete.")
    else:
        print("[WARNING] Not all models loaded. Server will run with limited functionality.")
except Exception as e:
    print(f"[FATAL ERROR] Critical failure during initial model loading: {e}")
    sys.exit(1)


# --- Middlewares and Routes (Rest of the code remains the same) ---

# =======================================================
# ðŸ“ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø£ÙˆÙ„: ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ (Ø¨Ø¯ÙˆÙ† Ù‚ÙŠÙˆØ¯)
# =======================================================
@app.route('/ai/generate_text', methods=['POST'])
def generate_text_unrestricted():
    # 1. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù…ÙØªØ§Ø­Ùƒ Ø§Ù„Ø³Ø±ÙŠ
    is_authenticated, response, status = authenticate_request()
    if not is_authenticated:
        return response, status

    if llama_model is None:
        return jsonify({
            "status": "error",
            "message": "Ù†ÙˆØ§Ø© Ø§Ù„Ù†ØµÙˆØµ ØºÙŠØ± Ù…ØªÙˆÙØ±Ø©. ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù†ØµÙŠ Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­."
        }), 503
    
    try:
        data = request.get_json()
        if not data or 'prompt' not in data:
            return jsonify({
                "status": "error",
                "message": "Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ Ù„Ù„ØªØ­Ù„ÙŠÙ„."
            }), 400
            
        user_prompt = data.get('prompt', 'Generate a detailed narrative.')
        max_tokens = min(int(data.get('max_tokens', 512)), 1024)  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ 1024 Ø±Ù…Ø²
        temperature = max(0.1, min(float(data.get('temperature', 0.7)), 1.0))  # Ù†Ø·Ø§Ù‚ 0.1 Ø¥Ù„Ù‰ 1.0

        try:
            output = llama_model.create_completion(
                user_prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=0.9,
                echo=False,
                stop=["\n###", "\n---", "\nØ§Ù„Ù…Ø³Ø§Ø¹Ø¯:"]
            )
            
            if not output or 'choices' not in output or not output['choices']:
                return jsonify({
                    "status": "error",
                    "message": "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ø³ØªØ¬Ø§Ø¨Ø© ØµØ§Ù„Ø­Ø© Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬."
                }), 500
                
            ai_result = output['choices'][0]['text'].strip()
            
            # Ø¥Ø²Ø§Ù„Ø© Ø£ÙŠ Ù†ØµÙˆØµ Ø²Ø§Ø¦Ø¯Ø© Ù‚Ø¯ ØªØ¸Ù‡Ø± Ø£Ø­ÙŠØ§Ù†Ø§Ù‹
            stop_sequences = ["\n###", "\n---", "\nØ§Ù„Ù…Ø³Ø§Ø¹Ø¯:", "\nHuman:", "\n###"]
            for seq in stop_sequences:
                if seq in ai_result:
                    ai_result = ai_result.split(seq)[0].strip()
            
            return jsonify({
                "status": "success",
                "output": ai_result,
                "model": "Dolphin-2.9-Llama3-8B",
                "tokens_generated": len(ai_result.split())
            }), 200
            
        except Exception as e:
            return jsonify({
                "status": "error",
                "message": f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø·Ù„Ø¨: {str(e)}",
                "error_type": str(type(e).__name__)
            }), 500

    except Exception as e:
        return jsonify({
            "status": "error",
            "message": f"Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {str(e)}",
            "error_type": str(type(e).__name__)
        }), 500

# =======================================================
# ðŸ–¼ï¸ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø«Ø§Ù†ÙŠ: ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØ± (Ø¨Ø¯ÙˆÙ† Ù‚ÙŠÙˆØ¯)
# =======================================================
@app.route('/ai/generate_image', methods=['POST'])
def generate_image_unrestricted():
    # 1. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù…ÙØªØ§Ø­Ùƒ Ø§Ù„Ø³Ø±ÙŠ
    is_authenticated, response, status = authenticate_request()
    if not is_authenticated: 
        return response, status

    if sd_pipeline is None:
        print("[ERROR] ØªÙ… Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ù†ÙˆØ§Ø© Ø§Ù„ØµÙˆØ± ÙˆÙ„ÙƒÙ†Ù‡Ø§ ØºÙŠØ± Ù…ØªÙˆÙØ±Ø©")
        return jsonify({
            "status": "error",
            "message": "Ù†ÙˆØ§Ø© Ø§Ù„ØµÙˆØ± ØºÙŠØ± Ù…ØªÙˆÙØ±Ø© Ø­Ø§Ù„ÙŠÙ‹Ø§. ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­.",
            "error_type": "ModelNotLoaded",
            "details": "The image generation model failed to load. Please check the server logs for more information."
        }), 503
        
    # 2. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
    try:
        print("[DEBUG] Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø§Ù„Ø© Ù†Ù…ÙˆØ°Ø¬ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØ±...")
        if sd_pipeline is None:
            raise RuntimeError("Ù†Ù…ÙˆØ°Ø¬ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØ± ØºÙŠØ± Ù…Ø­Ù…Ù„")
            
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ø¯ÙŠÙ‡ Ø§Ù„Ø³Ù…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        required_attrs = ['device', 'scheduler', 'text_encoder', 'vae', 'unet']
        for attr in required_attrs:
            if not hasattr(sd_pipeline, attr):
                raise RuntimeError(f"Ù†Ù…ÙˆØ°Ø¬ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØ± ØºÙŠØ± Ù…ÙƒØªÙ…Ù„: Ù…ÙÙ‚ÙˆØ¯ {attr}")
                
        print(f"[DEBUG] Ù†Ù…ÙˆØ°Ø¬ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØ± Ø¬Ø§Ù‡Ø² Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ù‡Ø§Ø²: {getattr(sd_pipeline, 'device', 'unknown')}")
        
    except Exception as e:
        error_msg = f"ÙØ´Ù„ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {str(e)}"
        print(f"[ERROR] {error_msg}")
        import traceback
        traceback.print_exc()
        return jsonify({
            "status": "error",
            "message": "Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØ±",
            "error_type": "ModelInitializationError",
            "details": str(e),
            "debug_info": {
                "pipeline_loaded": sd_pipeline is not None,
                "pipeline_attrs": dir(sd_pipeline) if sd_pipeline else []
            }
        }), 500
    
    try:
        data = request.get_json()
        if not data or 'prompt' not in data:
            return jsonify({
                "status": "error",
                "message": "Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ ÙˆØµÙ Ù„Ù„ØµÙˆØ±Ø© (prompt)."
            }), 400
            
        user_prompt = data.get('prompt', 'A detailed, photorealistic image.')
        num_inference_steps = min(int(data.get('num_inference_steps', 30)), 50)  # Limit to 50 steps max
        width = min(int(data.get('width', 512)), 1024)  # Max width 1024
        height = min(int(data.get('height', 512)), 1024)  # Max height 1024
        
        print(f"[INFO] Generating image with prompt: {user_prompt}")
        print(f"[INFO] Image dimensions: {width}x{height}, Steps: {num_inference_steps}")
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØ±Ø© Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
        try:
            print("[INFO] Starting image generation...")
            # Safely get device information
            device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"[DEBUG] Using device: {device}")
            
            # ØªØ¹Ø·ÙŠÙ„ xformers ÙÙŠ ÙˆØ¶Ø¹ CPU Ù„ØªÙØ§Ø¯ÙŠ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„
            try:
                if hasattr(sd_pipeline, 'enable_xformers_memory_efficient_attention'):
                    if torch.cuda.is_available():
                        sd_pipeline.enable_xformers_memory_efficient_attention()
                        print("[INFO] ØªÙ… ØªÙØ¹ÙŠÙ„ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„ÙØ¹Ø§Ù„Ø© Ù„Ù€ xformers")
                    else:
                        print("[INFO] ØªÙ… ØªØ¹Ø·ÙŠÙ„ xformers ÙÙŠ ÙˆØ¶Ø¹ CPU")
            except Exception as e:
                print(f"[WARNING] ØªØ¹Ø°Ø± ØªÙƒÙˆÙŠÙ† xformers: {e}")
                
            # ØªØ­Ø³ÙŠÙ†Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ù€ CPU
            if not torch.cuda.is_available():
                print("[INFO] ØªØ·Ø¨ÙŠÙ‚ ØªØ­Ø³ÙŠÙ†Ø§Øª ÙˆØ¶Ø¹ CPU...")
                try:
                    import torch
                    torch.set_num_threads(1)  # ØªÙ‚Ù„ÙŠÙ„ Ø¹Ø¯Ø¯ Ø§Ù„Ø®ÙŠÙˆØ· Ù„ØªØ¬Ù†Ø¨ Ø§Ø³ØªÙ†Ø²Ø§Ù Ø§Ù„Ù…ÙˆØ§Ø±Ø¯
                    if hasattr(sd_pipeline, 'enable_attention_slicing'):
                        sd_pipeline.enable_attention_slicing(slice_size='auto')
                        print("[INFO] ØªÙ… ØªÙØ¹ÙŠÙ„ ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ù„ØªÙˆÙÙŠØ± Ø§Ù„Ø°Ø§ÙƒØ±Ø©")
                except Exception as e:
                    print(f"[WARNING] ØªØ¹Ø°Ø± ØªØ·Ø¨ÙŠÙ‚ ØªØ­Ø³ÙŠÙ†Ø§Øª CPU: {e}")
            
            with torch.no_grad():
                try:
                    print("[INFO] Starting image generation...")
                    
                    # Ø¶Ø¨Ø· Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ±Ø© Ù„ÙŠÙƒÙˆÙ† Ù…Ù†Ø§Ø³Ø¨Ù‹Ø§ Ù„Ù„Ù€ CPU
                    # We are in a constrained CPU environment, force small size for safety.
                    max_size = 512 if torch.cuda.is_available() else 384 
                    gen_width = min(width, max_size)
                    gen_height = min(height, max_size)
                    
                    # ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ù…Ù† Ù…Ø¶Ø§Ø¹ÙØ§Øª 8
                    gen_width = (gen_width // 8) * 8
                    gen_height = (gen_height // 8) * 8
                    
                    print(f"[INFO] Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ±Ø© Ø¨Ø­Ø¬Ù… {gen_width}x{gen_height} Ù…Ø¹ {num_inference_steps} Ø®Ø·ÙˆØ©")
                    
                    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØµÙˆØ±Ø© Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù…Ø­Ø³Ù†Ø©
                    generation_kwargs = {
                        "prompt": user_prompt,
                        "negative_prompt": "blurry, low quality, distorted, bad anatomy, text, watermark, lowres, error",
                        "num_inference_steps": min(num_inference_steps, 30),  # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©
                        "guidance_scale": 7.5,
                        "width": gen_width,
                        "height": gen_height,
                        "num_images_per_prompt": 1,
                        "output_type": "pil"
                    }
                    
                    # ØªÙ†ÙÙŠØ° ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØ±Ø© Ù…Ø¹ ÙƒØ´Ù Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù…Ø­Ø³Ù†
                    try:
                        result = sd_pipeline(**generation_kwargs)
                    except Exception as gen_error:
                        print(f"[ERROR] ÙØ´Ù„ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØ±Ø©: {str(gen_error)}")
                        raise RuntimeError(f"ÙØ´Ù„ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØ±Ø©: {str(gen_error)}") from gen_error
                    
                    # Check if we got a valid result
                    if not result or not hasattr(result, 'images') or not result.images:
                        raise ValueError("No images were generated - invalid result format")
                    
                    image = result.images[0]
                    
                    # Verify the image
                    if not image:
                        raise ValueError("Generated image is empty")
                    
                    # Convert to RGB if needed
                    if hasattr(image, 'mode') and image.mode != 'RGB':
                        image = image.convert('RGB')
                    
                    print("[INFO] Image generated successfully")
                    
                except Exception as e:
                    error_msg = f"Image generation failed: {str(e)}"
                    print(f"[ERROR] {error_msg}")
                    import traceback
                    traceback.print_exc()
                    return jsonify({
                        "status": "error",
                        "message": f"ÙØ´Ù„ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØ±Ø©: {str(e)}",
                        "error_type": type(e).__name__,
                        "details": str(e)
                    }), 500
                
        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            print(f"[ERROR] Failed to generate image: {str(e)}")
            print(f"Error details: {error_details}")
            return jsonify({
                "status": "error",
                "message": f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØ±Ø©: {str(e)}",
                "error_type": str(type(e).__name__),
                "details": str(e)
            }), 500
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ base64
        import io
        import base64
        
        buffered = io.BytesIO()
        image.save(buffered, format="PNG")
        img_str = base64.b64encode(buffered.getvalue()).decode('utf-8')
        
        # Ø­ÙØ¸ Ø§Ù„ØµÙˆØ±Ø© Ù„Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø³Ø¬Ù„ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
        try:
            if not os.path.exists("output_images"):
                os.makedirs("output_images")
            filename = f"image_{secrets.token_urlsafe(8)}.png"
            image_path = os.path.join("output_images", filename)
            image.save(image_path)
        except Exception as e:
            print(f"Warning: Could not save image: {e}")
        
        return jsonify({
            "status": "success",
            "output": img_str,
            "message": "ØªÙ… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ù†Ø¬Ø§Ø­"
        }), 200

    except Exception as e:
        return jsonify({
            "message": f"Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {str(e)}",
            "error_type": str(type(e).__name__)
        }), 500

# =======================================================
# ðŸš€ Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ø®Ø¯Ù…Ø©
# =======================================================
@app.route('/health', methods=['GET'])
def health_check():
    """Health check endpoint to verify the server is running"""
    status = {
        "status": "running",
        "text_model_loaded": llama_model is not None,
        "image_model_loaded": sd_pipeline is not None,
        "system": {
            "python": sys.version.split()[0],
            "platform": sys.platform,
            "cuda_available": torch.cuda.is_available(),
            "device": "cuda" if torch.cuda.is_available() else "cpu"
        }
    }
    return jsonify(status), 200

# Add a root route for basic testing
@app.route('/')
def home():
    return """
    <h1>AI Server is Running</h1>
    <p>Available endpoints:</p>
    <ul>
        <li>GET /health - Check server status</li>
        <li>POST /ai/generate_text - Generate text</li>
        <li>POST /ai/generate_image - Generate images</li>
    </ul>
    """